package com.autoever.poc.adapters;

import java.io.IOException;
import java.nio.file.Files;
import java.nio.file.Path;
import java.nio.file.Paths;
import java.util.ArrayList;
import java.util.Arrays;
import java.util.List;

import com.autoever.poc.common.NumUtils;
import com.autoever.poc.common.RawDataField;
import com.autoever.poc.parser.PreProcessable;
import com.autoever.poc.parser.can.CanPreProcessor;
import com.autoever.poc.parser.ccp.CCPPreProcessor;
import com.streambase.sb.*;
import com.streambase.sb.operator.*;
import com.streambase.sb.util.Base64;

/**
 * Generated by JDT StreamBase Client Templates (Version: 11.0.1).
 *
 * This class is used as a Java Operator in a StreamBase application.
 * One instance will be created for each Java Operator in a StreamBase 
 * application. 
 * <p>
 * Enqueue methods should only be called from processTuple.
 * @see Parameterizable
 * @see Operator
 * For in-depth information on implementing a custom Java Operator, please see
 * "Developing StreamBase Java Operators" in the StreamBase documentation.
 */
public class VdmsRawParser extends Operator implements Parameterizable {

	public static final long serialVersionUID = 1685696973117L;
	// Properties
	private parserTypeEnum parserType;

	// Enum definition for property parserType 
	public static enum parserTypeEnum {
		CAN("can"), CCP("ccp"), GPS("gps");

		private final String rep;

		private parserTypeEnum(String s) {
			rep = s;
		}

		public String toString() {
			return rep;
		}
	}

	private String displayName = "Vdms Message Parser";
	// Local variables
	private int inputPorts = 1;
	private int outputPorts = 2;

	private Schema OutputSchema = null;
	private Schema StatusSchema = null;
	private PreProcessable preprocessor = null;

	/**
	* The constructor is called when the Operator instance is created, but before the Operator 
	* is connected to the StreamBase application. We recommended that you set the initial input
	* port and output port count in the constructor by calling setPortHints(inPortCount, outPortCount).
	* The default is 1 input port, 1 output port. The constructor may also set default values for 
	* operator parameters. These values will be displayed in StreamBase Studio when a new instance
	* of this operator is  dragged to the canvas, and serve as the default values for omitted
	* optional parameters.
	 */
	public VdmsRawParser() {
		super();
		setPortHints(inputPorts, outputPorts);
		setDisplayName(displayName);
		setShortDisplayName(this.getClass().getSimpleName());
		setParserType(parserTypeEnum.CAN);

	}

	/**
	* The typecheck method is called after the Operator instance is connected in the StreamBase
	* application, allowing the Operator to validate its properties. The Operator class may 
	* change the number of input or output ports by calling the requireInputPortCount(portCount)
	* method or the setOutputSchema(schema, portNum) method. If the verifyInputPortCount method 
	* is passed a different number of ports than the Operator currently has, a PortMismatchException
	* (subtype of TypecheckException) is thrown.
	*/
	public void typecheck() throws TypecheckException {
		// typecheck: require a specific number of input ports
		requireInputPortCount(inputPorts);

		// TODO Ensure that all properties have valid values, and typecheck the input schemas here

		try {
			Schema kafkaSchema = getNamedSchema("KafkaVDMSDataSchema");

			Schema inputSchema = getInputSchema(0);
			inputSchema.getField("kafkaMessage").checkType(CompleteDataType.forTuple(kafkaSchema));
			inputSchema.getField("binaryData").checkType(CompleteDataType.forBlob());
			inputSchema.getField("filePath").checkType(CompleteDataType.forString());

			Schema rawOutputSchema = getNamedSchema("VDMSBaseRaw");
			ArrayList<Schema.Field> outputSchemaField;// = new ArrayList<>(rawOutputSchema.fields());
			if(getParserType() == parserTypeEnum.CAN) {
				outputSchemaField = CanPreProcessor.getSchemaFields(rawOutputSchema);
			}else if(getParserType() == parserTypeEnum.CCP) {
				outputSchemaField = CCPPreProcessor.getSchemaFields(rawOutputSchema);
			}else {
				outputSchemaField = new ArrayList<>(rawOutputSchema.fields());
			}
			outputSchemaField.add(new Schema.Field("PassThroughs", CompleteDataType.forTuple(inputSchema)));
				
			Schema outputSchema = new Schema(null, outputSchemaField);


			List<Schema.Field> fields = List.of(
				new Schema.Field("TerminalID", CompleteDataType.forString()),
				new Schema.Field("MessageType", CompleteDataType.forInt()),
				new Schema.Field("GenCount", CompleteDataType.forInt())
			);
			Schema statusSchema = new Schema(null, fields);

			setOutputSchema(0, outputSchema);
			setOutputSchema(1, statusSchema);
		} catch(TupleException e) {
			throw new TypecheckException(e);
		}

	}

	/**
	* This method will be called by the StreamBase server for each Tuple given
	* to the Operator to process. This is the only time an operator should 
	* enqueue output Tuples.
	* @param inputPort the input port that the tuple is from (ports are zero based)
	* @param tuple the tuple from the given input port
	* @throws StreamBaseException Terminates the application.
	*/
	public void processTuple(int inputPort, Tuple tuple) throws StreamBaseException {
		if (inputPort > 0) {
			getLogger().info("operator skipping tuple at input port" + inputPort);
			return;
		}

		String filePath = tuple.getString("filePath");
		Tuple kafkaMessage = tuple.getTuple("kafkaMessage");
		ByteArrayView binData = null;
		try{
			binData = tuple.getBlobBuffer("binaryData");
		}catch(Exception e) {
			binData = null;
		}

		try {
			byte[] allBytes = null;
			if(binData != null && binData.length()>0) {
				allBytes = binData.array();
			}else if(filePath != null && !filePath.isEmpty()) {
				Path file = Paths.get(filePath);
				allBytes = Files.readAllBytes(file);
				Files.delete(file);
			}else {
				return;
			}

			if(allBytes != null && allBytes.length > 0) {
				List<Tuple> tuples = GetTuples(tuple, allBytes);
				if(tuples != null && !tuples.isEmpty()) {
					sendOutput(0, tuples);
					
					Tuple stuple = StatusSchema.createTuple();
					stuple.setField(0, kafkaMessage.getString("TerminalID"));
					stuple.setField(1, kafkaMessage.getInt("MessageType"));
					stuple.setField(2, tuples.size() -2);  // 2 : startTuple, endTuple 

					try {
						sendOutput(1, stuple);
					} catch (StreamBaseException e) {
						// TODO Auto-generated catch block
						e.printStackTrace();
					}
					
				}

			}
		} catch (IOException e1) {
			// TODO Auto-generated catch block
			e1.printStackTrace();
		} 
		
		
	}

	public List<Tuple> GetTuples(Tuple inputTuple, byte[] hcpMessage) {

		final int[] dlcSize = {0,1,2,3,4,5,6,7,8,12,16,20,24,32,48,64};
		final int headerSize = 10;	//IBBI(10byte)
		int sIndex = 0;
		
		List<Tuple> tuples = new ArrayList<>();

		try {
			int rawcount = hcpMessage.length;
			Tuple kafkaMessage = inputTuple.getTuple("kafkaMessage");
			long baseTime = kafkaMessage.getLong("BaseTime");

			if(preprocessor != null) {
				preprocessor.initialize(kafkaMessage);
			}
			
			Tuple startTuple = OutputSchema.createTuple();
			startTuple.setBoolean(RawDataField.IsStarted.getValue(), true);
			startTuple.setBoolean(RawDataField.IsEnded.getValue(), false);
			startTuple.setTuple("PassThroughs", inputTuple);
			tuples.add(startTuple);

			while(sIndex < rawcount) {

				int dlcIndex = NumUtils.getIntFromBig(hcpMessage, sIndex, 1);
				sIndex += 1;
				int curIndex =  sIndex;
				double deltaTime = (double)(NumUtils.getLongFromBig(hcpMessage, curIndex, 4) * 0.00005);
				curIndex += 4;
				int dataFlag = NumUtils.getIntFromBig(hcpMessage, curIndex, 1);
				curIndex += 1;
				int dataChannel = NumUtils.getIntFromBig(hcpMessage, curIndex, 1);
				curIndex += 1;
				int dataId = NumUtils.getIntFromBig(hcpMessage, curIndex, 4);
				curIndex += 4;
				int dataSize = dlcSize[dlcIndex];
				byte[] rawData = Arrays.copyOfRange(hcpMessage, curIndex, curIndex + dataSize);
				
				Object[] parsed = new Object[] { dataChannel, deltaTime, dataFlag, dataId, dlcIndex, rawData, baseTime, false, false };
				
				if(preprocessor != null) {
					preprocessor.preProcess(kafkaMessage, inputTuple, tuples, parsed, OutputSchema);
				} else {
					Tuple dataTuple = OutputSchema.createTuple();
					dataTuple.setInt(RawDataField.DataChannel.getValue(), dataChannel);
					dataTuple.setDouble(RawDataField.DeltaTime.getValue(), deltaTime);
					dataTuple.setInt(RawDataField.MSGInfo.getValue(), dataFlag);
					dataTuple.setInt(RawDataField.DataID.getValue(), dataId);
					dataTuple.setInt(RawDataField.DLC.getValue(), dlcIndex);
					dataTuple.setString(RawDataField.DATA.getValue(), Base64.encodeBytes(rawData));
					dataTuple.setLong(RawDataField.BaseTime.getValue(), baseTime);
					dataTuple.setBoolean(RawDataField.IsStarted.getValue(), false);
					dataTuple.setBoolean(RawDataField.IsEnded.getValue(), false);
					dataTuple.setTuple("PassThroughs", inputTuple);
						
					tuples.add(dataTuple);
				}
				
				/**
				Tuple dataTuple = OutputSchema.createTuple();
				dataTuple.setInt(RawDataField.DataChannel.getValue(), dataChannel);
				dataTuple.setDouble(RawDataField.DeltaTime.getValue(), deltaTime);
				dataTuple.setInt(RawDataField.MSGInfo.getValue(), dataFlag);
				dataTuple.setInt(RawDataField.DataID.getValue(), dataId);
				dataTuple.setInt(RawDataField.DLC.getValue(), dlcIndex);
//				dataTuple.setString(RawDataField.DATA.getValue(), Base64.encodeBytes(rawData));
				dataTuple.setLong(RawDataField.BaseTime.getValue(), baseTime);
				dataTuple.setBoolean(RawDataField.IsStarted.getValue(), false);
				dataTuple.setBoolean(RawDataField.IsEnded.getValue(), false);
				dataTuple.setTuple("PassThroughs", inputTuple);
					
				if(preprocessor != null && !preprocessor.preProcess(dataTuple, rawData, param)) {
				}else {
					// for speedup.
					dataTuple.setString(RawDataField.DATA.getValue(), Base64.encodeBytes(rawData));
					tuples.add(dataTuple);
				}
				**/

				sIndex += (dataSize + headerSize);
			}

			Tuple endTuple = OutputSchema.createTuple();
			endTuple.setBoolean(RawDataField.IsStarted.getValue(), false);
			endTuple.setBoolean(RawDataField.IsEnded.getValue(), true);
			endTuple.setTuple("PassThroughs", inputTuple);
			tuples.add(endTuple);

		}catch(Exception e) {
			System.out.println("RawParser Exception:" + e.getMessage());
			return tuples;
		}
		
		return tuples;
	}
	
	
	/**
	 * If typecheck succeeds, the init method is called before the StreamBase application
	 * is started. Note that your Operator class is not required to define the init method,
	 * unless (for example) you need to perform initialization of a resource such as a JDBC
	 * pool, if your operator is making JDBC calls. StreamBase Studio does not call this
	 * during authoring.
	 */
	public void init() throws StreamBaseException {
		super.init();

		if(parserType == parserTypeEnum.CAN) {
			preprocessor = new CanPreProcessor();
		}else if(parserType == parserTypeEnum.CCP){
			preprocessor = new CCPPreProcessor();
		}else {
			preprocessor = null;
		}

		OutputSchema = getRuntimeOutputSchema(0);
		StatusSchema = getRuntimeOutputSchema(1);

	}

	/**
	*  The shutdown method is called when the StreamBase server is in the process of shutting down.
	*/
	public void shutdown() {

	}

	/***************************************************************************************
	 * The getter and setter methods provided by the Parameterizable object.               *
	 * StreamBase Studio uses them to determine the name and type of each property         *
	 * and obviously, to set and get the property values.                                  *
	 ***************************************************************************************/

	public void setParserType(parserTypeEnum parserType) {
		this.parserType = parserType;
	}

	public parserTypeEnum getParserType() {
		return this.parserType;
	}

	/** For detailed information about shouldEnable methods, see interface Parameterizable java doc 
	 *  @see Parameterizable 
	 */

	public boolean shouldEnableParserType() {
		// TODO implement custom enablement logic here
		return true;
	}

}
